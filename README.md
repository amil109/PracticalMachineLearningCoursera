# Practical Machine Learning Assignment #


Code used to decide on the model selected for prediction of the dumbell lift type ("classe" variable in the data set) can be found in the github repository below:

[INSERT LINK TO YOUR R CODE]

In order to select the model, I first checked which columns contained values in the test set (without looking at what the valid values were), and filtered the data in the training set to include only these values, as including any parameters in the model that are null in the target data does not make sense.

The resulting filtered data was then split into a test and training set, and the training set split into 5 folds for use in cross validation.

I then used the ggplot2 package to create density plots of the different variables in the training data, in order to get an idea about how the data looked. Not all plots generated are shown below, but all are of the same type, and can be generated by substituting a different variable name in the 'x' variable for the plot.

```{r}
library(ggplot2)
qplot(roll_arm, colour = classe, data = pmltraining, geom = "density")
```

Observing the range of x values in the different plots suggested that it would be beneficial to normalise the data before training or applying a model. I verified this by checking the means of various columns using the aggregate function (again, not all tables are shown, but can be generated by substituting the term in the grep expression).

```{r}
aggregate(pmltraining[,grep("_arm",colnames(pmltraining))], list(pmltraining$classe), FUN=mean)
```

Because the data is continuous, a model based approach seemed appropriate. 

"lm", "glm", "lda", "qda" and "nb" were all tried on the cross-validation sets in the testing data using a for-loop to loop over the sets.

```
for (i in 1:length(trainfolds)){
  mod <- train(classe ~ ., data=pmltraining[trainfolds[[i]],], 
                  preProcess = c("center", "scale"), method = "qda")
  pred <- predict(mod, pmltraining[testfolds[[i]],])
  print(confusionMatrix(pred,pmltraining[testfolds[[i]],]$classe))
}
```

Results were as follows:
"lm" and "glm" both throw errors, which is understandable, since the relationship certainly appears non-linear (but it doesn't hurt to try them!).

"nb" was not able to complete in a reasonable amount of time on my machine when using all variables, so I concluded that this method was too computationally intense for these purposes, and mvoed on. It seems likely that this method would obtain good results given the nature of the data, although there are a number of variables that are clearly connected (as they are separate measurements from the same sensor), so it may be that this method is excessive for this data set.

"lda" and "qda" both returned results. Linear discriminant analysis ("lda") showed an accuracy of around 70% ([0.7038, 0.6993, 0.7089, 0.6969, 0.7137] for the 5 folds) during cross-validation. Quadratic discriminant analysis ("qda") performed better, with an accuracy of around 90% ([0.9025, 0.8916, 0.8947, 0.9059, 0.8981] for the 5 folds) during cross-validation.

I tried using principal components analysis to reduce the dimensionality of the model, using the code below:

```{r}
ctrl <- trainControl(preProcOptions = list(thresh = 0.95))
mod <- train(classe ~ ., data=pmltraining[trainfolds[[i]],], 
                  preProcess = c("center", "scale", "pca"), trControl = ctrl, method = "qda")
confusionMatrix(pred,pmltraining[testfolds[[i]],]$classe)
```

This was done with a view to increasing the speed of analysis, however, using threshold values of 0.9 and 0.95, the accuracy obtained with the "qda" model fell to around 66% and 74% respectively, and the computing time was not noticeably reduced.

With the above in mind, I selected "qda" as the model, and since it was able to be computed in a reasonably short time, decided not to reduce the dimensionality by using principal components analysis ("pca"). This gives the final model, which we apply to the test set just once.

```{r}
finalmod <- train(classe ~ ., data=pmltraining, 
                  preProcess = c("center", "scale"), method = "qda")
finalpred <- predict(finalmod, pmltesting)
print(confusionMatrix(finalpred,pmltesting$classe))
```

As above, the out-of-sample error is estimated to have a 95% confidence interval of between 88.8% and 90.52%.

Note that for application of the model to the separate testing set for upload (the 20 examples), the code at the end of the R-script was used. This uses the exact same logic as the previous test set.

